from bs4 import BeautifulSoup
import requests
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def search_stackoverflow(query: str):
    url = "https://api.stackexchange.com/2.3/search/advanced"
    params = {
        "order": "desc",
        "sort": "relevance",
        "q": query,
        "site": "stackoverflow",
        "pagesize": 5,
    }
    response = requests.get(url, params=params)
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")
    
    data = response.json()
    combined_text = []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(data['items'])}")  # –û—Ç–ª–∞–¥–∫–∞
    for item in data["items"][:len(data['items'])]:
        try:
            url = item["link"]
            response = requests.get(url)
            if response.status_code != 200:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url}")
                continue
            
            soup = BeautifulSoup(response.text, 'lxml')
            question = soup.find('div', class_="s-prose js-post-body")
            right_answer = soup.find('div', class_="answercell post-layout--right")
            
            if right_answer:
                text = right_answer.find('div', class_="s-prose js-post-body")
                if text and text.get_text(strip=True):
                    combined_text.append(f"Title: {item['title']}\nLink: {item['link']}\nQuestion: {question.get_text()}\nAnswer: {text.get_text(strip=True)}")
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {url}")  # –û—Ç–ª–∞–¥–∫–∞
                else:
                    print(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {url}")
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {url}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text


def search_habr(query: str):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    search_url = f"https://habr.com/ru/search/?q={query}&target_type=posts"
    driver.get(search_url)
    time.sleep(3)  # –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è

    soup = BeautifulSoup(driver.page_source, 'lxml')
    posts = soup.find_all("li", class_="tm-articles-list__item")

    articles = []
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")

    for post in posts:
        try:
            link_tag = post.find("a", class_="tm-title__link")
            if not link_tag:
                continue

            title = link_tag.text.strip()
            href = "https://habr.com" + link_tag["href"]

            # –ü–∞—Ä—Å–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–π–∫–æ–≤
            rating_tag = post.find("span", class_="tm-votes-meter__value")
            rating = int(rating_tag.text.strip()) if rating_tag else 0

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏
            driver.get(href)
            time.sleep(2)
            article_soup = BeautifulSoup(driver.page_source, "lxml")
            article_body = article_soup.find("div", id="post-content-body")
            if not article_body:
                continue

            content = article_body.get_text(separator="\n", strip=True)
            articles.append({
                "title": title,
                "link": href,
                "content": content,
                "likes": rating
            })

            print(f"–î–æ–±–∞–≤–ª–µ–Ω –ø–æ—Å—Ç: {title} | üëç {rating}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")

    driver.quit()

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ª–∞–π–∫–∞–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 5
    top_articles = sorted(articles, key=lambda x: x["likes"], reverse=True)[:5]

    combined_text = [
        f"Title: {a['title']}\nLink: {a['link']}\nLikes: {a['likes']}\nContent: {a['content']}"
        for a in top_articles
    ]

    print(f"–í—ã–±—Ä–∞–Ω–æ —Ç–æ–ø-{len(combined_text)} –ø–æ—Å—Ç–æ–≤ –ø–æ –ª–∞–π–∫–∞–º")
    return combined_text


def search_reddit(query: str):
    url = "https://www.reddit.com/r/all/search.json"
    params = {
        "q": query,
        "sort": "relevance", 
        "limit": 5, 
    }
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
    
    response = requests.get(url, params=params, headers=headers)
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")
    
    data = response.json()
    combined_text = []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(data['data']['children'])}")  # –û—Ç–ª–∞–¥–∫–∞
    for item in data["data"]["children"]:
        try:
            post_data = item["data"]
            post_url = f"https://www.reddit.com{post_data['permalink']}"
            post_title = post_data["title"]
            post_text = post_data["selftext"]
            post_comments = post_data["num_comments"]
            
            combined_text.append(f"Title: {post_title}\nLink: {post_url}\nText: {post_text}\nComments: {post_comments}")
            print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {post_url}")  
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}") 
    return combined_text

def search_mailru(query: str):
    url = f"https://www.habr.com/search/?q={query}"
    
    soup = BeautifulSoup(url, 'lxml')

    # –ò—â–µ–º –ø–æ—Å—Ç—ã –ø–æ data-testid (–∫–∞–∫ –≤ –≤–∞—à–µ–º –ø–µ—Ä–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ)
    posts = soup.find_all('div', class_="")
    combined_text = []

    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(posts)}")  # –û—Ç–ª–∞–¥–∫–∞
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text

def search_geekforgeeks(query: str):
    # URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ GeekForGeeks
    base_url = "https://www.geeksforgeeks.org"
    search_url = f"{base_url}/search/?q={query}"

    # –û—Ç–ø—Ä–∞–≤–∫–∞ GET-–∑–∞–ø—Ä–æ—Å–∞
    response = requests.get(search_url)

    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")

    soup = BeautifulSoup(response.text, 'lxml')

    # –ù–∞–π–¥–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ (–ø–µ—Ä–µ–¥–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏)
    search_results = soup.find_all('div', class_='gsc-webResult')

    combined_text = []

    for item in search_results:
        try:
            title = item.find('a', class_='gs-title')
            link = title['href']
            title_text = title.get_text(strip=True)
            
            # –ü–æ–ª—É—á–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–ª–∏ –≤–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
            description = item.find('div', class_='gs-snippet')
            description_text = description.get_text(strip=True) if description else '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

            combined_text.append(f"Title: {title_text}\nLink: {link}\nDescription: {description_text}")
            print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {link}")  # –û—Ç–ª–∞–¥–∫–∞
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")

    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text
