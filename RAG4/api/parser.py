from bs4 import BeautifulSoup
import requests
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def search_stackoverflow(query: str):
    url = "https://api.stackexchange.com/2.3/search/advanced"
    params = {
        "order": "desc",
        "sort": "relevance",
        "q": query,
        "site": "stackoverflow",
        "pagesize": 5,
    }
    response = requests.get(url, params=params)
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")
    
    data = response.json()
    combined_text = []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(data['items'])}")  # –û—Ç–ª–∞–¥–∫–∞
    for item in data["items"][:len(data['items'])]:
        try:
            url = item["link"]
            response = requests.get(url)
            if response.status_code != 200:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url}")
                continue
            
            soup = BeautifulSoup(response.text, 'lxml')
            question = soup.find('div', class_="s-prose js-post-body")
            right_answer = soup.find('div', class_="answercell post-layout--right")
            
            if right_answer:
                text = right_answer.find('div', class_="s-prose js-post-body")
                if text and text.get_text(strip=True):
                    combined_text.append(f"Title: {item['title']}\nLink: {item['link']}\nQuestion: {question.get_text()}\nAnswer: {text.get_text(strip=True)}")
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {url}")  # –û—Ç–ª–∞–¥–∫–∞
                else:
                    print(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {url}")
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {url}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text


def search_habr(query: str):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    search_url = f"https://habr.com/ru/search/?q={query}&target_type=posts&sort=relevance"
    driver.get(search_url)
    time.sleep(3)  # –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è

    soup = BeautifulSoup(driver.page_source, 'lxml')
    posts = soup.find_all("div", class_="tm-article-snippet tm-article-snippet")

    articles = []
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")

    post_counter=0
    for post in posts:
        post_counter+=1
        if post_counter > 5:
            break
        try:
            link_tag = post.find("a", class_="tm-title__link")
            if not link_tag:
                continue

            title = link_tag.text.strip()
            href = "https://habr.com" + link_tag["href"]

            # –ü–∞—Ä—Å–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–π–∫–æ–≤
            rating_tag = post.find("span", class_="tm-votes-meter__value")
            rating = int(rating_tag.text.strip()) if rating_tag else 0

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏
            driver.get(href)
            time.sleep(2)
            article_soup = BeautifulSoup(driver.page_source, "lxml")
            article_body = article_soup.find("div", id="post-content-body")
            if not article_body:
                continue

            content = article_body.get_text(separator="\n", strip=True)
            articles.append({
                "title": title,
                "link": href,
                "content": content,
                "likes": rating
            })

            print(f"–î–æ–±–∞–≤–ª–µ–Ω –ø–æ—Å—Ç: {title} | üëç {rating}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")

    driver.quit()

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ª–∞–π–∫–∞–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 5
    top_articles = sorted(articles, key=lambda x: x["likes"], reverse=True)[:5]

    combined_text = [
        f"Title: {a['title']}\nLink: {a['link']}\nLikes: {a['likes']}\nContent: {a['content']}"
        for a in top_articles
    ]

    print(f"–í—ã–±—Ä–∞–Ω–æ —Ç–æ–ø-{len(combined_text)} –ø–æ—Å—Ç–æ–≤ –ø–æ –ª–∞–π–∫–∞–º")
    return combined_text


def search_reddit(query: str):
    url = "https://www.reddit.com/r/all/search.json"
    params = {
        "q": query,
        "sort": "relevance", 
        "limit": 5, 
    }
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
    
    response = requests.get(url, params=params, headers=headers)
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")
    
    data = response.json()
    combined_text = []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(data['data']['children'])}")  # –û—Ç–ª–∞–¥–∫–∞
    for item in data["data"]["children"]:
        try:
            post_data = item["data"]
            post_url = f"https://www.reddit.com{post_data['permalink']}"
            post_title = post_data["title"]
            post_text = post_data["selftext"]
            post_comments = post_data["num_comments"]
            
            combined_text.append(f"Title: {post_title}\nLink: {post_url}\nText: {post_text}\nComments: {post_comments}")
            print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {post_url}")  
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}") 
    return combined_text

def search_mailru(query: str):
    url = f"https://otvet.mail.ru/search/{query}"

    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)
    
    driver.get(url)
    time.sleep(3)  # –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è

    soup = BeautifulSoup(driver.page_source, 'lxml')
    # –ò—â–µ–º –ø–æ—Å—Ç—ã –ø–æ data-testid (–∫–∞–∫ –≤ –≤–∞—à–µ–º –ø–µ—Ä–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ)
    posts = soup.find_all('div', class_="mMhMm")
    
    combined_text = []
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(posts)}")  # –û—Ç–ª–∞–¥–∫–∞
    for post in posts:
        try:
            post=post.find('a', class_="KFtEM aR6dQ Ub4yk")
            post_url = f"https://otvet.mail.ru{post['href']}"
            post_title = post.get_text()
            print(post_title)
            driver.get(post_url)

            soup = BeautifulSoup(driver.page_source, 'lxml')
            question = soup.find('div', class_="aitWd PcSgH")
            print("\nQuestion", question)
            right_answer = soup.find('div', class_="aitWd _Jzbh")
            print("\nRight_answer", right_answer)
            if right_answer:
                combined_text.append(f"Title: {post_title}\nLink: {post_url}\nQuestion: {question.get_text()}\nAnswer: {right_answer.get_text()}")
                print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {url}")  # –û—Ç–ª–∞–¥–∫–∞
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {url}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Å—Ç–∞: {e}")
    
    
    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text

def search_geekforgeeks(query: str):
    search_url = f"https://www.geeksforgeeks.org/search/?q={query}"

    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    driver.get(search_url)
    time.sleep(3)  

    soup = BeautifulSoup(driver.page_source, 'lxml')
    
    search_results = soup.find_all('div', class_="gcse-title")

    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ API: {len(search_results)}")
    combined_text = []

    for item in search_results:
        try:
            title = item.find('div', class_="article-title")
            link = title['href']
            title_text = title.get_text(strip=True)

            response = requests.get(link)

            soup = BeautifulSoup(response, 'lxml')
            discription = soup.find('div', class_="article--viewer_content")
            print("\nDiscription", discription)
            if discription:
                combined_text.append(f"Title: {title_text}\nLink: {link}\nDiscription: {discription}")
                print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {link}")  # –û—Ç–ª–∞–¥–∫–∞
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è {link}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")

    print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤: {combined_text}")  # –û—Ç–ª–∞–¥–∫–∞
    return combined_text
